{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from suite2p import run_s2p\n",
    "from suite2p.io import BinaryFile\n",
    "from suite2p.detection import bin_movie\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175459b4",
   "metadata": {},
   "source": [
    "run to get intermediate data during detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = Path('/media/carsen/disk2/suite2p_paper/GT1/')\n",
    "settings = np.load(root / 'suite2p/plane1/settings.npy', allow_pickle=True).item()\n",
    "db = np.load(root / 'suite2p/plane1/db.npy', allow_pickle=True).item()\n",
    "reg_outputs = np.load(root / 'suite2p/plane1/reg_outputs.npy', allow_pickle=True).item()\n",
    "reg_file = root / 'suite2p/plane1/data.bin'\n",
    "\n",
    "tau = settings['tau']\n",
    "fs = settings['fs']\n",
    "Ly, Lx = db['Ly'], db['Lx']\n",
    "with BinaryFile(Ly=Ly, Lx=Lx, filename=reg_file) as f_reg:\n",
    "    n_frames, Ly, Lx = f_reg.shape\n",
    "    yrange = reg_outputs['yrange']\n",
    "    xrange = reg_outputs['xrange']\n",
    "    badframes = reg_outputs['badframes']\n",
    "    nbins = settings['detection']['nbins']\n",
    "    bin_size = int(max(1, n_frames // nbins, np.round(tau * fs)))\n",
    "    mov = bin_movie(f_reg, bin_size, yrange=yrange, xrange=xrange,\n",
    "                        badframes=badframes, nbins=nbins)\n",
    "\n",
    "mov_ex = mov[:100].copy()\n",
    "print(mov.shape)\n",
    "\n",
    "tau = settings['tau']\n",
    "fs = settings['fs']\n",
    "Ly, Lx = db['Ly'], db['Lx']\n",
    "with BinaryFile(Ly=Ly, Lx=Lx, filename=reg_file) as f_reg:\n",
    "    n_frames, Ly, Lx = f_reg.shape\n",
    "    yrange = reg_outputs['yrange']\n",
    "    xrange = reg_outputs['xrange']\n",
    "    badframes = reg_outputs['badframes']\n",
    "    nbins = settings['detection']['nbins']\n",
    "    bin_size = int(max(1, n_frames // nbins, np.round(tau * fs)))\n",
    "    mov = bin_movie(f_reg, bin_size, yrange=yrange, xrange=xrange,\n",
    "                        badframes=badframes, nbins=nbins)\n",
    "\n",
    "mov_ex = mov[:100].copy()\n",
    "\n",
    "from suite2p.detection import utils\n",
    "from suite2p.detection.sparsedetect import neuropil_subtraction\n",
    "\n",
    "mov = utils.temporal_high_pass_filter(mov=mov, width=settings['detection'][\"highpass_time\"])\n",
    "sdmov = utils.standard_deviation_over_time(mov, batch_size=1000)\n",
    "mov = neuropil_subtraction(\n",
    "        mov=mov / sdmov,\n",
    "        filter_size=settings['detection']['sparsery_settings']['highpass_neuropil'])  # subtract low-pass filtered movie\n",
    "\n",
    "mov_filt_ex = mov.copy()\n",
    "\n",
    "from suite2p.detection.sparsedetect import square_convolution_2d, downsample\n",
    "_, Lyc, Lxc = mov.shape\n",
    "LL = np.meshgrid(np.arange(Lxc), np.arange(Lyc))\n",
    "gxy = [np.array(LL).astype(\"float32\")]\n",
    "dmov = mov\n",
    "movu = []\n",
    "\n",
    "# downsample movie at various spatial scales\n",
    "Lyp, Lxp = np.zeros(5, \"int32\"), np.zeros(5, \"int32\")  # downsampled sizes\n",
    "for j in range(5):\n",
    "    movu0 = square_convolution_2d(dmov, 3)\n",
    "    dmov = 2 * downsample(dmov)\n",
    "    gxy0 = downsample(gxy[j], False)\n",
    "    gxy.append(gxy0)\n",
    "    _, Lyp[j], Lxp[j] = movu0.shape\n",
    "    movu.append(movu0)\n",
    "\n",
    "from suite2p.detection.sparsedetect import find_best_scale, threshold_reduce\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "spatial_scale = settings['detection']['sparsery_settings']['spatial_scale']\n",
    "threshold_scaling = settings['detection']['threshold_scaling']\n",
    "\n",
    "# spline over scales\n",
    "I = np.zeros((len(gxy), gxy[0].shape[1], gxy[0].shape[2]))\n",
    "for movu0, gxy0, I0 in zip(movu, gxy, I):\n",
    "    gmodel = RectBivariateSpline(gxy0[1, :, 0], gxy0[0, 0, :], movu0.max(axis=0),\n",
    "                                    kx=min(3, gxy0.shape[1] - 1),\n",
    "                                    ky=min(3, gxy0.shape[2] - 1))\n",
    "    I0[:] = gmodel(gxy[0][1, :, 0], gxy[0][0, 0, :])\n",
    "v_corr = I.max(axis=0)\n",
    "\n",
    "scale, estimate_mode = find_best_scale(I=I, spatial_scale=spatial_scale)\n",
    "# TODO: scales from cellpose (?)\n",
    "#    scales = 3 * 2 ** np.arange(5.0)\n",
    "#    scale = np.argmin(np.abs(scales - diam))\n",
    "#    estimate_mode = EstimateMode.Estimated\n",
    "\n",
    "spatscale_pix = 3 * 2**scale\n",
    "mask_window = int(((spatscale_pix * 1.5) // 2) * 2)\n",
    "Th2 = threshold_scaling * 5 * max(\n",
    "    1, scale)  # threshold for accepted peaks (scale it by spatial scale)\n",
    "vmultiplier = max(1, mov.shape[0] / 1200)\n",
    "print(\"NOTE: %s spatial scale ~%d pixels, time epochs %2.2f, threshold %2.2f \" %\n",
    "        (estimate_mode.value, spatscale_pix, vmultiplier, vmultiplier * Th2))\n",
    "\n",
    "# get standard deviation for pixels for all values > Th2\n",
    "v_map = [threshold_reduce(movu0, Th2) for movu0 in movu]\n",
    "\n",
    "from suite2p.detection.sparsedetect import add_square, iter_extend, two_comps, multiscale_mask, extendROI\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "max_ROIs = 10000\n",
    "active_percentile = 0\n",
    "\n",
    "movu = [movu0.reshape(movu0.shape[0], -1) for movu0 in movu]\n",
    "\n",
    "mov = np.reshape(mov, (-1, Lyc * Lxc))\n",
    "lxs = 3 * 2**np.arange(5)\n",
    "nscales = len(lxs)\n",
    "\n",
    "v_max = np.zeros(max_ROIs)\n",
    "ihop = np.zeros(max_ROIs)\n",
    "v_split = np.zeros(max_ROIs)\n",
    "V1 = deepcopy(v_map)\n",
    "stats = []\n",
    "patches = []\n",
    "seeds = []\n",
    "extract_patches = False\n",
    "\n",
    "t0 = time.time()\n",
    "ypix_all = []\n",
    "xpix_all = []\n",
    "lam_all = []\n",
    "tproj_all = []\n",
    "active_frames_all = []\n",
    "f_init = []\n",
    "for tj in range(max_ROIs):\n",
    "    # find peaks in stddev\"s\n",
    "    v0max = np.array([V1[j].max() for j in range(5)])\n",
    "    imap = np.argmax(v0max)\n",
    "    imax = np.argmax(V1[imap])\n",
    "    yi, xi = np.unravel_index(imax, (Lyp[imap], Lxp[imap]))\n",
    "    # position of peak\n",
    "    yi, xi = gxy[imap][1, yi, xi], gxy[imap][0, yi, xi]\n",
    "    med = [int(yi), int(xi)]\n",
    "\n",
    "    # check if peak is larger than threshold * max(1,nbinned/1200)\n",
    "    v_max[tj] = v0max.max()\n",
    "    if v_max[tj] < vmultiplier * Th2:\n",
    "        break\n",
    "    ls = lxs[imap]\n",
    "\n",
    "    ihop[tj] = imap\n",
    "\n",
    "    # make square of initial pixels based on spatial scale of peak\n",
    "    yi, xi = int(yi), int(xi)\n",
    "    ypix0, xpix0, lam0 = add_square(yi, xi, ls, Lyc, Lxc)\n",
    "\n",
    "    # project movie into square to get time series\n",
    "    tproj = (mov[:, ypix0 * Lxc + xpix0] * lam0[0]).sum(axis=-1)\n",
    "    f_init.append(tproj.copy())\n",
    "    if active_percentile > 0:\n",
    "        threshold = min(Th2, np.percentile(tproj, active_percentile))\n",
    "    else:\n",
    "        threshold = Th2\n",
    "    active_frames = np.nonzero(tproj > threshold)[0]  # frames with activity > Th2\n",
    "\n",
    "    # get square around seed\n",
    "    if extract_patches:\n",
    "        mask = mov[active_frames].mean(axis=0).reshape(Lyc, Lxc)\n",
    "        patches.append(utils.square_mask(mask, mask_window, yi, xi))\n",
    "        seeds.append([yi, xi])\n",
    "\n",
    "    # extend mask based on activity similarity\n",
    "    ypixs, xpixs = [ypix0.copy()], [xpix0.copy()]\n",
    "    lams = [np.ones(len(ypix0), 'float32')]\n",
    "        \n",
    "    for j in range(3):\n",
    "        #ypix0, xpix0, lam0 = iter_extend(ypix0, xpix0, mov, Lyc, Lxc, active_frames)\n",
    "        npix = 0\n",
    "        iter = 0\n",
    "        ypix, xpix = ypix0.copy(), xpix0.copy()\n",
    "        while npix < 10000:\n",
    "            npix = ypix.size\n",
    "            # extend ROI by 1 pixel on each side\n",
    "            ypix, xpix = extendROI(ypix, xpix, Lyc, Lxc, 1)\n",
    "            # activity in proposed ROI on ACTIVE frames\n",
    "            usub = mov[np.ix_(active_frames, ypix * Lxc + xpix)]\n",
    "            lam = np.mean(usub, axis=0)\n",
    "            ix = lam > max(0, lam.max() / 5.0)\n",
    "            if ix.sum() == 0:\n",
    "                break\n",
    "            ypix, xpix, lam = ypix[ix], xpix[ix], lam[ix]\n",
    "            if iter == 0:\n",
    "                sgn = 1.\n",
    "            if np.sign(sgn * (ix.sum() - npix)) <= 0:\n",
    "                break\n",
    "            else:\n",
    "                npix = ypix.size\n",
    "            iter += 1\n",
    "            ypixs.append(ypix.copy())\n",
    "            xpixs.append(xpix.copy())\n",
    "            lams.append(lam.copy())\n",
    "        lam = lam / np.sum(lam**2)**.5\n",
    "        ypix0, xpix0, lam0 = ypix.copy(), xpix.copy(), lam.copy()\n",
    "        tproj = mov[:, ypix0 * Lxc + xpix0] @ lam0\n",
    "        active_frames = np.nonzero(tproj > threshold)[0]\n",
    "        if len(active_frames) < 1:\n",
    "            #if tj < max_ROIs/2: # TODO: nmasks is undefined\n",
    "            #    continue\n",
    "            #else:\n",
    "            break\n",
    "    \n",
    "    if len(active_frames) < 1:\n",
    "        #if tj < max_ROIs/2:\n",
    "        #    continue\n",
    "        #else:\n",
    "        break\n",
    "\n",
    "    ypix_all.append(ypixs)\n",
    "    xpix_all.append(xpixs)\n",
    "    lam_all.append(lams)\n",
    "    \n",
    "    # check if ROI should be split\n",
    "    v_split[tj], ipack = two_comps(mov[:, ypix0 * Lxc + xpix0], lam0, threshold)\n",
    "    if v_split[tj] > 1.25:\n",
    "        lam0, xp, active_frames = ipack\n",
    "        tproj[active_frames] = xp\n",
    "        ix = lam0 > lam0.max() / 5\n",
    "        xpix0 = xpix0[ix]\n",
    "        ypix0 = ypix0[ix]\n",
    "        lam0 = lam0[ix]\n",
    "        ymed = np.median(ypix0)\n",
    "        xmed = np.median(xpix0)\n",
    "        imin = np.argmin((xpix0 - xmed)**2 + (ypix0 - ymed)**2)\n",
    "        med = [ypix0[imin], xpix0[imin]]\n",
    "\n",
    "    # update residual on raw movie\n",
    "    mov[np.ix_(active_frames,\n",
    "                ypix0 * Lxc + xpix0)] -= tproj[active_frames][:, np.newaxis] * lam0\n",
    "    # update filtered movie\n",
    "    ys, xs, lms = multiscale_mask(ypix0, xpix0, lam0, Lyp, Lxp)\n",
    "    for j in range(nscales):\n",
    "        movu[j][np.ix_(active_frames, xs[j] + Lxp[j] * ys[j])] -= np.outer(\n",
    "            tproj[active_frames], lms[j])\n",
    "        Mx = movu[j][:, xs[j] + Lxp[j] * ys[j]]\n",
    "        V1[j][ys[j], xs[j]] = (Mx**2 * np.float32(Mx > threshold)).sum(axis=0)**.5\n",
    "\n",
    "    tproj_all.append(tproj)\n",
    "    active_frames_all.append(active_frames)\n",
    "\n",
    "    stats.append({\n",
    "        \"ypix\": ypix0.astype(int),\n",
    "        \"xpix\": xpix0.astype(int),\n",
    "        \"lam\": lam0 * sdmov[ypix0, xpix0],\n",
    "        \"med\": med,\n",
    "        \"footprint\": ihop[tj]\n",
    "    })\n",
    "\n",
    "    if tj % 500 == 0:\n",
    "        t1 = time.time() - t0\n",
    "        print(f\"ROIs: {tj},\\t last score: {v_max[tj]:0.4f}, \\t time: {t1:0.2f}sec\")\n",
    "\n",
    "import cv2\n",
    "v_map_rsz = [cv2.resize(vm, (Lxc, Lyc), interpolation=cv2.INTER_LINEAR) \n",
    "             for vm in v_map]\n",
    "mov = mov.reshape(-1, Lyc, Lxc)\n",
    "ihop = ihop[:len(stats)]\n",
    "\n",
    "masks_overlap = np.zeros((Lyc, Lxc), 'int')\n",
    "masks = np.zeros((Lyc, Lxc), 'uint16')\n",
    "masks_all = [np.zeros((Lyc, Lxc), 'uint16') for i in range(len(v_map))]\n",
    "iperm = np.random.permutation(len(stats))\n",
    "for n in range(len(stats)-1, -1, -1):\n",
    "    ypix, xpix = stats[n]['ypix'], stats[n]['xpix']\n",
    "    #overlap = stat[n]['overlap']\n",
    "    #ypix, xpix = ypix[~overlap], xpix[~overlap]\n",
    "    masks_all[int(ihop[n])][ypix, xpix] = iperm[n]+1\n",
    "    masks_overlap[ypix, xpix] += 1\n",
    "    masks[ypix, xpix] = n+1 #iperm[n]+1#n+1\n",
    "\n",
    "ioverlap = np.unique(masks[10:50, 320:360])[1:] - 1\n",
    "                     #[110:135, 135:165])[1:] - 1 \n",
    "#215:235, -89:-80])[1:] - 1\n",
    "print(ioverlap)\n",
    "print([(masks_overlap[stats[i]['ypix'], stats[i]['xpix']] > 1).mean() for i in ioverlap])\n",
    "\n",
    "overlap = np.zeros((Lyc, Lxc), 'uint16')\n",
    "mask_id = np.zeros((len(ioverlap), Lyc, Lxc), 'bool')\n",
    "mask_pic = np.ones((Lyc, Lxc, 3), 'float32')\n",
    "colors = plt.get_cmap('tab20')\n",
    "colors = plt.get_cmap('gist_ncar')(np.linspace(0, 0.9, len(ioverlap)))[::-1]\n",
    "traces = np.zeros((len(ioverlap), len(mov)))\n",
    "for i, ic in enumerate(ioverlap):\n",
    "    overlap[stats[ic]['ypix'], stats[ic]['xpix']] += 1 \n",
    "    mask_id[i, stats[ic]['ypix'], stats[ic]['xpix']] = True\n",
    "    mask_pic[mask_id[i]] = colors[i][:3]\n",
    "    f = tproj_all[ic].copy()\n",
    "    f -= f.min()\n",
    "    f /= f.max()\n",
    "    traces[i, active_frames_all[ic]] = f[active_frames_all[ic]]\n",
    "\n",
    "for iy, ix in zip(np.nonzero(overlap > 1)[0], np.nonzero(overlap > 1)[1]):\n",
    "    ii = np.nonzero(mask_id[:, iy, ix])[0]\n",
    "    col = np.array([colors[i][:3] for i in ii]).mean(axis=0)\n",
    "    col += 0.1\n",
    "    col = np.minimum(1., col)\n",
    "    mask_pic[iy, ix] = col#ors(i)[:3]\n",
    "    \n",
    "#pic[overlap > 1] = 0.8 * np.ones(3)    \n",
    "intersection = ((traces > 0).astype('int') @ (traces > 0).astype('int').T).astype('float32')\n",
    "union = (traces > 0).sum(axis=1) + (traces > 0).sum(axis=1)[:,np.newaxis] - intersection #/ ((traces > 0) + (traces > 0).T)\n",
    "iou = intersection / union\n",
    "iou[np.triu_indices(iou.shape[0], k=0)[0], np.triu_indices(iou.shape[0], k=0)[1]] = np.nan\n",
    "\n",
    "\n",
    "# olims = [[199, 236], [528, 561]]\n",
    "olims = [[np.nonzero(mask_id.sum(axis=(0,2)))[0][0], Lyc - np.nonzero(mask_id.sum(axis=(0,2))[::-1])[0][0]-1],\n",
    "         [np.nonzero(mask_id.sum(axis=(0,1)))[0][0], Lxc - np.nonzero(mask_id.sum(axis=(0,1))[::-1])[0][0]-1]]\n",
    "olims[0][1] -= 20\n",
    "olims[1][0] += 0\n",
    "olims[1][1] -= 20\n",
    "print(olims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/ex_detect.npy', {'mov': mov_ex, 'mov_filt': mov_filt_ex, 'v_map': v_map_rsz, \n",
    "                                  'ypix_all': ypix_all, 'xpix_all': xpix_all, \n",
    "                                  'lam_all': lam_all, 'f_init': f_init, 'threshold': threshold, \n",
    "                                  'masks_all': masks_all, 'mask_pic': mask_pic, 'mask_id': mask_id,\n",
    "                                  'iou': iou, 'traces': traces, 'colors': colors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.load('results/ex_detect.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c623a",
   "metadata": {},
   "source": [
    "### figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54739c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import figures\n",
    "import importlib \n",
    "importlib.reload(figures)\n",
    "\n",
    "ylim = [0, 450]\n",
    "xlim = [300, 622]\n",
    "fig = figures.detection_fig(ylim, xlim, **dat)\n",
    "fig.savefig('figures/fig4.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73879ef6",
   "metadata": {},
   "source": [
    "# benchmark\n",
    "\n",
    "first run suite2p for registration and `max_proj`, and extract cells using Cellpose from `max_proj` image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from suite2p import default_settings, default_db, parameters\n",
    "import torch \n",
    "import detect_benchmarks \n",
    "from suite2p import run_s2p\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# set to path to TIFFS\n",
    "root = Path(\"/media/carsen/disk1/suite2p_paper/VG23/2025_08_06/1/\")\n",
    "n_planes = 4\n",
    "settings = default_settings()\n",
    "db = default_db()\n",
    "import json\n",
    "with open(root / \"ops.json\", \"r\") as f:\n",
    "    settings_in = json.load(f)\n",
    "\n",
    "db, settings, settings_in = parameters.convert_settings_orig(settings_in, \n",
    "                                                          db=db, \n",
    "                                                          settings=settings)\n",
    "\n",
    "db['data_path'] = [str(root)]\n",
    "db[\"save_path0\"] = str(root) \n",
    "db[\"nplanes\"] = n_planes\n",
    "settings['tau'] = 0.25\n",
    "settings['run']['multiplane_parallel'] = False\n",
    "settings['io']['delete_bin'] = False\n",
    "\n",
    "print(db['fast_disk'])\n",
    "\n",
    "run_s2p(db=db, settings=settings);\n",
    "\n",
    "# remove baseline fluorescence\n",
    "for ipl in range(n_planes):\n",
    "    detect_benchmarks.baseline_movie(root, ipl=ipl)\n",
    "\n",
    "# downsample planes spatially\n",
    "for ipl in range(n_planes):\n",
    "    detect_benchmarks.downsample_movie(root, ipl=ipl, ds=2, do_filt=True)\n",
    "\n",
    "### cellpose segmentation on full movies\n",
    "# get traces from downsampled orig movie and baselined movie\n",
    "for ipl in range(4):\n",
    "    db = np.load(root / \"suite2p\" / f\"plane{ipl}\" / \"db.npy\", allow_pickle=True).item()\n",
    "    Ly, Lx = db[\"Ly\"], db[\"Lx\"]\n",
    "    reg_outputs = np.load(root / \"suite2p\" / f\"plane{ipl}\" / \"reg_outputs.npy\", allow_pickle=True).item()\n",
    "    yrange, xrange = reg_outputs[\"yrange\"], reg_outputs[\"xrange\"]\n",
    "    meanImg = reg_outputs[\"meanImg\"]\n",
    "    detect_outputs = np.load(root / \"suite2p\" / f\"plane{ipl}\" / \"detect_outputs.npy\", allow_pickle=True).item()\n",
    "    max_proj0 = detect_outputs[\"max_proj\"]\n",
    "    max_proj = np.zeros((Ly, Lx), dtype=\"float32\")\n",
    "    max_proj[yrange[0] : yrange[1], xrange[0] : xrange[1]] = max_proj0\n",
    "    meanImg[:yrange[0], :] = 0\n",
    "    meanImg[yrange[1]:, :] = 0\n",
    "    meanImg[:, :xrange[0]] = 0\n",
    "    meanImg[:, xrange[1]:] = 0\n",
    "\n",
    "    img = max_proj.copy()\n",
    "\n",
    "    masks, stat_gt, F_gt, Fneu_gt = detect_benchmarks.cellpose_extract(img, Ly, Lx, \n",
    "                                                     [root / \"suite2p_ds\" / f\"plane{ipl}\" / \"data.bin\", \n",
    "                                                     root / \"suite2p_ds\" / f\"plane{ipl}\" / \"data_filt.bin\"], ds=2)\n",
    "    print(masks.max())\n",
    "\n",
    "    (root / \"benchmarks\").mkdir(exist_ok=True)\n",
    "    np.save(root / \"benchmarks\" / f\"stat_gt_plane{ipl}.npy\", stat_gt)\n",
    "    np.save(root / \"benchmarks\" / f\"F_gt_plane{ipl}.npy\", F_gt[0])\n",
    "    np.save(root / \"benchmarks\" / f\"F_gt_filt_plane{ipl}.npy\", F_gt[1])\n",
    "    np.save(root / \"benchmarks\" / f\"Fneu_gt_plane{ipl}.npy\", Fneu_gt[0])\n",
    "    np.save(root / \"benchmarks\" / f\"Fneu_gt_filt_plane{ipl}.npy\", Fneu_gt[1])\n",
    "    np.save(root / \"benchmarks\" / f\"masks_gt_plane{ipl}.npy\", masks)\n",
    "    np.save(root / \"benchmarks\" / f\"img_gt_plane{ipl}.npy\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedf98f",
   "metadata": {},
   "source": [
    "### generate hybrid ground-truth + run suite2p\n",
    "\n",
    "For the `threshold_scaling` sweep, run the following in the env, changing path to binaries for each mouse to folder containing `suite2p_ds`:\n",
    "```\n",
    "python detect_benchmarks.py --root /path/to/bin/ --param_sweep --iplane 0\n",
    "python detect_benchmarks.py --root /path/to/bin/ --param_sweep --iplane 1\n",
    "python detect_benchmarks.py --root /path/to/bin/ --param_sweep --iplane 2\n",
    "python detect_benchmarks.py --root /path/to/bin/ --param_sweep --iplane 3\n",
    "```\n",
    "\n",
    "Then for the sweep across different simulation parameters, run:\n",
    "```\n",
    "python detect_benchmarks.py --root /path/to/bin/ --sweep --iplane 0\n",
    "python detect_benchmarks.py --root /path/to/bin/ --sweep --iplane 1\n",
    "python detect_benchmarks.py --root /path/to/bin/ --sweep --iplane 2\n",
    "python detect_benchmarks.py --root /path/to/bin/ --sweep --iplane 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e9185",
   "metadata": {},
   "source": [
    "### run caiman\n",
    "\n",
    "Create an env with caiman:\n",
    "\n",
    "```\n",
    "conda create --name caiman caiman -c conda-forge -y\n",
    "conda activate caiman\n",
    "pip install natsort\n",
    "```\n",
    "\n",
    "For the $K$ num components sweep, run the following in the env, changing path to binaries for each mouse:\n",
    "```\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep --iplane 0\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep --iplane 1\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep --iplane 2\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep --iplane 3\n",
    "```\n",
    "\n",
    "For the grid sweep, run the following:\n",
    "```\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep_grid --iplane 0\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep_grid --iplane 1\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep_grid --iplane 2\n",
    "python detect_caiman.py --root /path/to/bin/ --param_sweep_grid --iplane 3\n",
    "```\n",
    "\n",
    "\n",
    "Then for the sweep across different simulations, run:\n",
    "```\n",
    "python detect_caiman.py --root /path/to/bin/ --sweep --iplane 0\n",
    "python detect_caiman.py --root /path/to/bin/ --sweep --iplane 1\n",
    "python detect_caiman.py --root /path/to/bin/ --sweep --iplane 2\n",
    "python detect_caiman.py --root /path/to/bin/ --sweep --iplane 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc2034",
   "metadata": {},
   "source": [
    "### compute optimal threshold / comps for suite2p / caiman\n",
    "\n",
    "and create supp fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdf3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = np.arange(0.6, 1.5, 0.1)\n",
    "\n",
    "tps = np.zeros((3, 4, len(ths)))\n",
    "fps = np.zeros((3, 4, len(ths)))\n",
    "fns = np.zeros((3, 4, len(ths)))\n",
    "n_ell, neu_coeff, poisson_coeff = 2000, 0.4, 20\n",
    "\n",
    "for im, mname in enumerate(['VG21', 'VG23', 'VG24']):\n",
    "    root = Path(f'/home/carsen/dm11_string/suite2p_paper/hybrid_gt/{mname}/')\n",
    "    for iplane in range(4):\n",
    "        for i, th in enumerate(ths):\n",
    "            rstr = f's2p_{th:.1f}'\n",
    "            filename = root / 'sims' / f'results_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy'\n",
    "            # print(filename)\n",
    "            if filename.exists():\n",
    "                #print(f'Loading {filename}')\n",
    "                tp, fp, fn, f1 = np.load(filename)\n",
    "                tps[im, iplane, i] = tp\n",
    "                fps[im, iplane, i] = fp\n",
    "                fns[im, iplane, i] = fn\n",
    "\n",
    "f1s = tps / (tps + 0.5*(fns+fps))\n",
    "print(f1s.mean(axis=(0, 1)))\n",
    "\n",
    "metrics_s2p = [f1s, tps, fns, fps]\n",
    "n_ell, neu_coeff, poisson_coeff = 2000, 0.4, 20\n",
    "K = np.array([7,8,9,10,11,12,13])\n",
    "tps = np.zeros((3, 4, len(K)))\n",
    "fps = np.zeros((3, 4, len(K)))\n",
    "fns = np.zeros((3, 4, len(K)))\n",
    "for im, mname in enumerate(['VG21', 'VG23', 'VG24']):\n",
    "    root = Path(f'/home/carsen/dm11_string/suite2p_paper/hybrid_gt/{mname}/')\n",
    "    for iplane in range(4):\n",
    "        for i, K0 in enumerate(K):\n",
    "            rstr = f'caiman_K_{K0}_p_1_gnb_2_gSig0_5_rf_15'\n",
    "            #print(rstr)\n",
    "            filename = root / 'sims' / f'results_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy'\n",
    "            # print(filename)\n",
    "            if filename.exists():\n",
    "                #print(f'Loading {filename}')\n",
    "                tp, fp, fn, f1 = np.load(filename)\n",
    "                tps[im, iplane, i] = tp\n",
    "                fps[im, iplane, i] = fp\n",
    "                fns[im, iplane, i] = fn\n",
    "            else:\n",
    "                print(filename)\n",
    "\n",
    "f1s = tps / (tps + 0.5*(fns+fps))\n",
    "\n",
    "metrics_caiman = [f1s, tps, fns, fps]\n",
    "\n",
    "print(f1s.mean(axis=(0,1)))\n",
    "\n",
    "from fig_utils import *\n",
    "fig = plt.figure(figsize=(14,3), dpi=150)\n",
    "grid = plt.GridSpec(1, 6, wspace=0.5, hspace=0.5, figure=fig, \n",
    "                            bottom=0.2, top=0.88, left=0.05, right=0.99)\n",
    "\n",
    "ylabels = ['F1 score', 'false negatives', 'false positives']\n",
    "il = 0\n",
    "transl = mtransforms.ScaledTranslation(-40/72, 5/72, fig.dpi_scale_trans)\n",
    "xlabels = ['threshold scaling', '# of components (K)']\n",
    "for i, (params, metrics) in enumerate(zip([ths, K], [metrics_s2p, metrics_caiman])):\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(grid[0, 3*i + j])\n",
    "        metric = metrics[j + 1*(j>0)].copy()\n",
    "        mmean = metric.mean(axis=(0,1))\n",
    "        merr = metric.std(axis=(0,1)) / (np.prod(metric.shape[:2])-1)**0.5\n",
    "        ax.errorbar(params, mmean, merr, color=alg_cols[i])\n",
    "        if j==0:\n",
    "            ibest = mmean.argmax()\n",
    "            ax.set_ylim([0, 0.9])\n",
    "            il = plot_label(ltr, il, ax, transl) \n",
    "        else:\n",
    "            ax.set_ylim([0, 1000])\n",
    "        ax.scatter(params[ibest], mmean[ibest], marker='*', color='k', \n",
    "                       s=100, alpha=1, zorder=0)\n",
    "        ax.set_ylabel(ylabels[j])\n",
    "        ax.set_xlabel(xlabels[i])\n",
    "        if i==1:\n",
    "            ax.set_xticks([8, 10, 12])\n",
    "        if j==0:\n",
    "            ax.set_title(alg_names[i][:1].upper()+alg_names[i][1:], color=alg_cols[i],\n",
    "                         loc='left')\n",
    "fig.savefig('figures/suppfig_thK.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107c780",
   "metadata": {},
   "source": [
    "### aggregate TPs, FPs, FNs from caiman parameter grid sweep\n",
    "\n",
    "and create supp fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps_all = np.zeros((3,4,2,3,3,3,3))\n",
    "fps_all = np.zeros((3,4,2,3,3,3,3))\n",
    "fns_all = np.zeros((3,4,2,3,3,3,3))\n",
    "\n",
    "n_ell, neu_coeff, poisson_coeff = 2000, 0.4, 20\n",
    "for im, mname in enumerate(['VG21', 'VG23', 'VG24']):\n",
    "    root = Path(f'/home/carsen/dm11_string/suite2p_paper/hybrid_gt/{mname}/')\n",
    "\n",
    "    for iplane in range(4):\n",
    "        tps, fps, fns = [], [], []\n",
    "        for p in [1, 2]:\n",
    "            for gnb in [1, 2, 3]:\n",
    "                for K in [6, 9, 12]:\n",
    "                    for gSig0 in [3, 5, 7]:\n",
    "                        for rf in [12, 15, 18]:\n",
    "                            rstr = f'caiman_K_{K}_p_{p}_gnb_{gnb}_gSig0_{gSig0}_rf_{rf}'\n",
    "                            filename = root / 'sims' / f'results_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy'\n",
    "                            if filename.exists():\n",
    "                                #print(f'Loading {filename}')\n",
    "                                tp, fp, fn, f1 = np.load(filename)\n",
    "                                tps.append(tp)\n",
    "                                fps.append(fp)\n",
    "                                fns.append(fn)\n",
    "                            else:\n",
    "                                print(filename)\n",
    "        tps = np.array(tps)\n",
    "        fps = np.array(fps)\n",
    "        fns = np.array(fns)\n",
    "\n",
    "        tps = tps.reshape(2,3,3,3,3)\n",
    "        fps = fps.reshape(2,3,3,3,3)\n",
    "        fns = fns.reshape(2,3,3,3,3)\n",
    "        tps_all[im, iplane] = tps\n",
    "        fps_all[im, iplane] = fps\n",
    "        fns_all[im, iplane] = fns\n",
    "\n",
    "from fig_utils import *\n",
    "\n",
    "param_names = ['autoreg order (p)', '# of background comps. (gnb)', \n",
    "               '# of components (K)', 'gaussian smoothing (gSig)', 'patch half-size (rf)']\n",
    "params = [[1, 2], [1, 2, 3], [6, 9, 12], [3, 5, 7], [12, 15, 18]]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8), dpi=150)\n",
    "grid = plt.GridSpec(3, 5, wspace=0.4, hspace=0.1, figure=fig, \n",
    "                            bottom=0.05, top=0.92, left=0.07, right=0.99)\n",
    "\n",
    "f1s_all = tps_all.copy() / (tps_all + 0.5 * (fps_all + fns_all))\n",
    "metrics_orig = [[metrics_s2p[k][:,:,metrics_s2p[0].mean(axis=(0,1)).argmax()].flatten(), \n",
    "                 metrics_caiman[k][:,:,metrics_caiman[0].mean(axis=(0,1)).argmax()].flatten()] \n",
    "                 for k in [0, 2, 3]]\n",
    "il = 0\n",
    "transl = mtransforms.ScaledTranslation(-40/72, 0/72, fig.dpi_scale_trans)\n",
    "for i in range(5):\n",
    "    ni = np.arange(2, 7)\n",
    "    ni = np.delete(ni, i)\n",
    "    freshape = f1s_all.copy().transpose(0, 1, i+2, *ni).reshape(np.prod(f1s_all.shape[:2]), f1s_all.shape[i+2], -1)\n",
    "    print(freshape.shape)\n",
    "    imax = freshape.mean(axis=0).argmax(axis=-1)\n",
    "    nl = freshape.shape[1]\n",
    "    for k in range(3):\n",
    "        ax = plt.subplot(grid[k, i])\n",
    "        if k==0:\n",
    "            pos = ax.get_position().bounds \n",
    "            ax.set_position([pos[0], pos[1]+0.04, *pos[2:]])\n",
    "        if k==1:\n",
    "            freshape = fns_all.copy().transpose(0, 1, i+2, *ni).reshape(np.prod(f1s_all.shape[:2]), f1s_all.shape[i+2], -1)\n",
    "        elif k==2:\n",
    "            freshape = fps_all.copy().transpose(0, 1, i+2, *ni).reshape(np.prod(f1s_all.shape[:2]), f1s_all.shape[i+2], -1)\n",
    "        fmax = freshape[:, np.arange(nl), imax]\n",
    "        fmean = fmax.mean(axis=0)\n",
    "        nerr = (fmax.shape[0] - 1)**0.5\n",
    "        ferr = fmax.std(axis=0) / nerr\n",
    "        ax.errorbar(params[i], fmean, ferr, color='k')\n",
    "        for j, metric in enumerate(metrics_orig[k]):\n",
    "            ax.plot(params[i], metric.mean() * np.ones(nl), color=alg_cols[j], ls='--', zorder=30, lw=2)\n",
    "        if i==2:\n",
    "            ax.set_xticks([6, 8, 10, 12])\n",
    "        elif i==0:\n",
    "            ax.set_xticks([1, 2])\n",
    "            ax.set_ylabel(ylabels[k])\n",
    "        if k==0:\n",
    "            ax.set_xlabel(param_names[i])\n",
    "            ax.set_ylim([0.3, 0.8])\n",
    "            il = plot_label(ltr, il, ax, transl) \n",
    "        else:\n",
    "            ax.set_ylim([0, 720])\n",
    "        if i==0 and k==0:\n",
    "            ax.text(0.05, 0.83, alg_names[0][:1].upper()+alg_names[0][1:], color=alg_cols[0],\n",
    "                    transform=ax.transAxes)\n",
    "            ax.text(0.05, 0.04, alg_names[1][:1].upper()+alg_names[1][1:] + \n",
    "                    '\\n(p=1, gnb=2, K=9, \\n gSig=5, rf=15)', \n",
    "                    color=alg_cols[1], transform=ax.transAxes)\n",
    "\n",
    "fig.savefig('figures/suppfig_sweep.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a32a23",
   "metadata": {},
   "source": [
    "### aggregate TPs, FPs, FNs for all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "n_ells = np.arange(0, 4001, 500)\n",
    "neu_coeffs = np.arange(0, 0.81, 0.1)\n",
    "poisson_coeffs = [0, 5, 10, 20, 50, 100, 200]\n",
    "\n",
    "tps = [np.zeros((3, 4, 2, len(var))) for var in [n_ells, neu_coeffs, poisson_coeffs]]\n",
    "fps = [np.zeros((3, 4, 2, len(var))) for var in [n_ells, neu_coeffs, poisson_coeffs]]\n",
    "fns = [np.zeros((3, 4, 2, len(var))) for var in [n_ells, neu_coeffs, poisson_coeffs]]\n",
    "\n",
    "for im, mname in enumerate(['VG21', 'VG23', 'VG24']):\n",
    "    root = Path(f'/home/carsen/dm11_string/suite2p_paper/hybrid_gt/{mname}/')\n",
    "    for j, var in enumerate([n_ells, neu_coeffs, poisson_coeffs]):\n",
    "            \n",
    "        for iplane in range(4):\n",
    "            \n",
    "            # default params\n",
    "            n_ell, neu_coeff, poisson_coeff = 2000, 0.4, 20\n",
    "            \n",
    "            for i, v in enumerate(var):\n",
    "                if j==0:\n",
    "                    n_ell = v \n",
    "                elif j==1:\n",
    "                    neu_coeff = v\n",
    "                elif j==2:\n",
    "                    poisson_coeff = v\n",
    "                \n",
    "                for k, rstr in enumerate(['s2p_0.7', 'caiman']):\n",
    "                    filename = root / 'sims' / f'results_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy'\n",
    "                    \n",
    "                    if filename.exists():\n",
    "                        #print(f'Loading {filename}')\n",
    "                        tp, fp, fn, f1 = np.load(filename)\n",
    "                        tps[j][im, iplane, k, i] = tp\n",
    "                        fps[j][im, iplane, k, i] = fp\n",
    "                        fns[j][im, iplane, k, i] = fn\n",
    "\n",
    "idef = np.nonzero(n_ells == 2000)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9211a926",
   "metadata": {},
   "source": [
    "### simulation example + overlap for figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/media/carsen/disk1/suite2p_paper/VG23/2025_08_06/1/')\n",
    "iplane = 1\n",
    "db = np.load(root / 'suite2p' / f'plane{iplane}' / 'db.npy', allow_pickle=True).item()\n",
    "reg_outputs = np.load(root / 'suite2p' / f'plane{iplane}' / 'reg_outputs.npy', allow_pickle=True).item()\n",
    "yrange, xrange = reg_outputs['yrange'], reg_outputs['xrange']\n",
    "Ly, Lx = db['Ly'], db['Lx']\n",
    "\n",
    "cp_masks = np.load(root / 'benchmarks' / f'masks_gt_plane{iplane}.npy')\n",
    "cp_masks = cp_masks[yrange[0]:yrange[1], xrange[0]:xrange[1]]\n",
    "F_gt = np.load(root / 'benchmarks' / f'F_gt_plane{iplane}.npy')\n",
    "Fneu_gt = np.load(root / 'benchmarks' / f'Fneu_gt_plane{iplane}.npy')\n",
    "stat_gt = np.load(root / 'benchmarks' / f'stat_gt_plane{iplane}.npy', allow_pickle=True)\n",
    "dF_gt = F_gt - 0.7 * Fneu_gt\n",
    "snr_gt = 1 - 0.5 * np.diff(dF_gt, axis=1).var(axis=1) / dF_gt.var(axis=1)\n",
    "npix_gt = np.array([len(s['ypix']) for s in stat_gt])\n",
    "# filter ground-truth and predicted ROIs\n",
    "min_size = np.percentile(npix_gt, 5)\n",
    "max_size = np.percentile(npix_gt, 95)\n",
    "# snr_threshold = 0.25\n",
    "igood_gt = (snr_gt > 0.25) * (npix_gt > min_size) * (npix_gt < max_size)\n",
    "dF_gt = dF_gt[igood_gt]\n",
    "\n",
    "f_ex = []\n",
    "for ipl in range(4):\n",
    "    detect_outputs = np.load(root / 'suite2p' / f'plane{ipl}' / 'detect_outputs.npy', allow_pickle=True).item()\n",
    "    f_ex.append(detect_outputs['max_proj'])\n",
    "    if ipl==iplane:\n",
    "        max_proj = f_ex[-1].copy()\n",
    "\n",
    "minY = np.array([f_ex[i].shape[0] for i in range(len(f_ex))]).min()\n",
    "minX = np.array([f_ex[i].shape[1] for i in range(len(f_ex))]).min()\n",
    "f_ex = [f_ex[i][:minY, :minX] for i in range(len(f_ex))]\n",
    "\n",
    "\n",
    "from cellpose.utils import outlines_list\n",
    "cp_outlines = outlines_list(cp_masks)\n",
    "\n",
    "import detect_benchmarks    \n",
    "import torch \n",
    "# defaults\n",
    "n_ell = 2000\n",
    "neu_coeff = 0.4\n",
    "poisson_coeff = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "stat_ell, F_ell = detect_benchmarks.make_dendrites(root, n_planes=4, iplane=iplane, ds=2, n_ell=n_ell)\n",
    "masks_ell = np.zeros((Ly//2, Lx//2), 'int')\n",
    "for i in range(len(stat_ell)):\n",
    "    ypix, xpix = stat_ell[i]['ypix'], stat_ell[i]['xpix']\n",
    "    masks_ell[ypix, xpix] += 1\n",
    "S, Fneu_sim = detect_benchmarks.make_neuropil(root, n_planes=4, iplane=ipl, device=device)\n",
    "Sr = S.reshape(S.shape[0], -1)\n",
    "Sr = torch.from_numpy(Sr).to(device).float()\n",
    "with BinaryFile(Ly=Ly//2, Lx=Lx//2, \n",
    "                    filename=root / 'suite2p_ds' / f'plane{iplane}' / 'data_filt.bin',\n",
    "                    ) as f_reg:\n",
    "    tstart, tend = 3000, 3100\n",
    "    data = np.array(f_reg[tstart : tend])\n",
    "\n",
    "    fr_ell = np.zeros(data.shape, 'float32')\n",
    "    for i in range(len(stat_ell)):\n",
    "        ypix, xpix, lam = stat_ell[i]['ypix'], stat_ell[i]['xpix'], stat_ell[i]['lam']\n",
    "        F_ell0 = F_ell[i, tstart : tend, np.newaxis] * (lam / (lam).sum()) * len(ypix)\n",
    "        fr_ell[:, ypix, xpix] += F_ell0\n",
    "\n",
    "    fr_ell = torch.from_numpy(fr_ell).to(device) \n",
    "    fr_ell = torch.clamp(fr_ell, 0)\n",
    "\n",
    "    dmean = data.mean()  # compute on first batch\n",
    "    \n",
    "    Fsim = torch.from_numpy(Fneu_sim).to(device).float()\n",
    "    dadd = Fsim[:, tstart : tend].T @ Sr \n",
    "    dadd = dadd.reshape(-1, Ly//2, Lx//2)\n",
    "    dadd += dmean\n",
    "    dadd = torch.clamp(dadd, 0)    \n",
    "    \n",
    "    d_out = torch.from_numpy(data).to(device).float()\n",
    "    \n",
    "    d_out *= 1 - neu_coeff\n",
    "    d_out += neu_coeff * dadd\n",
    "\n",
    "    d_out += fr_ell * (1 - neu_coeff)\n",
    "\n",
    "    d_out = torch.poisson(d_out / poisson_coeff) * poisson_coeff\n",
    "    \n",
    "    d_out = torch.clamp(d_out, 0, 65534//2).int().cpu().numpy()\n",
    "\n",
    "neu_ex = dadd.cpu().numpy()\n",
    "ell_ex = fr_ell.cpu().numpy()\n",
    "\n",
    "from cellpose.utils import outlines_list, masks_to_outlines\n",
    "\n",
    "# default params\n",
    "n_ell, neu_coeff, poisson_coeff = 2000, 0.4, 20\n",
    "\n",
    "iplane = 1\n",
    "root = Path(f'/home/carsen/dm11_string/suite2p_paper/hybrid_gt/VG23/')\n",
    "db = np.load(root / \"suite2p_ds\" / f\"plane{iplane}\" / \"db.npy\", allow_pickle=True).item()\n",
    "n_frames, Ly, Lx = db[\"nframes\"], db[\"Ly\"], db[\"Lx\"]\n",
    "reg_outputs = np.load(root / \"suite2p_ds\" / f\"plane{iplane}\" / \"reg_outputs.npy\", allow_pickle=True).item()\n",
    "yrange, xrange = reg_outputs[\"yrange\"], reg_outputs[\"xrange\"]\n",
    "    \n",
    "# load ground-truth ROIs\n",
    "stat_gt = np.load(root / \"benchmarks\" / f\"stat_gt_plane{iplane}.npy\", allow_pickle=True)\n",
    "F_gt = np.load(root / \"benchmarks\" / f\"F_gt_plane{iplane}.npy\", allow_pickle=True)\n",
    "Fneu_gt = np.load(root / \"benchmarks\" / f\"Fneu_gt_plane{iplane}.npy\", allow_pickle=True)\n",
    "dF_gt = F_gt.copy() - 0.7 * Fneu_gt    \n",
    "\n",
    "dFs = []\n",
    "stats = []\n",
    "\n",
    "rstr = 's2p_0.7'\n",
    "stats.append(np.load(root / 'sims' / f'stat_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy', allow_pickle=True))\n",
    "F = np.load(root / 'sims' / f'F_{rstr}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy')\n",
    "Fneu = np.load(root / 'sims' / f'Fneu_0.7_s2p_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}.npy')\n",
    "dFs.append(F - 0.7 * Fneu)\n",
    "\n",
    "for param_type0 in ['caiman']:\n",
    "    run_name0 = f'{param_type0}_neu_{neu_coeff:.2f}_ell_{n_ell}_poisson_{poisson_coeff}_plane{iplane}'\n",
    "    dFs.append(np.load(root / 'sims' / f\"F_{run_name0}.npy\"))\n",
    "    stat = np.load(root / 'sims' / f\"stat_{run_name0}.npy\", allow_pickle=True)\n",
    "    lam_threshold = 0.1\n",
    "    for i in range(len(stat)):\n",
    "        lam = stat[i][\"lam\"]\n",
    "        stat[i][\"ypix\"] = stat[i][\"ypix\"][lam > lam_threshold * lam.max()]\n",
    "        stat[i][\"xpix\"] = stat[i][\"xpix\"][lam > lam_threshold * lam.max()]\n",
    "        stat[i]['ypix'] += yrange[0]\n",
    "        stat[i]['xpix'] += xrange[0]\n",
    "        stat[i][\"lam\"] = stat[i][\"lam\"][lam > lam_threshold * lam.max()]\n",
    "    stats.append(stat)\n",
    "\n",
    "snr_threshold = 0.25\n",
    "snr_gt = 1 - 0.5 * np.diff(dF_gt, axis=1).var(axis=1) / dF_gt.var(axis=1)\n",
    "npix_gt = np.array([len(s['ypix']) for s in stat_gt])\n",
    "# filter ground-truth and predicted ROIs\n",
    "min_size = np.percentile(npix_gt, 5)\n",
    "max_size = np.percentile(npix_gt, 95)\n",
    "igood_gt = (snr_gt > snr_threshold) * (npix_gt > min_size) * (npix_gt < max_size)    \n",
    "stat_gt = stat_gt[igood_gt]\n",
    "dF_gt = dF_gt[igood_gt]\n",
    "\n",
    "igoods = []\n",
    "for dF, stat in zip(dFs, stats):\n",
    "    snr = 1 - 0.5 * np.diff(dF, axis=1).var(axis=1) / dF.var(axis=1)\n",
    "    npix = np.array([len(s['ypix']) for s in stat])\n",
    "    igood = (snr > snr_threshold) * (npix > min_size) * (npix < max_size)\n",
    "    print(igood.sum())\n",
    "    igoods.append(igood)\n",
    "\n",
    "stats = [stat[igood] for stat, igood in zip(stats, igoods)]\n",
    "    \n",
    "masks_gt = np.zeros((Ly, Lx), 'uint16')\n",
    "for i in range(len(stat_gt)):\n",
    "    ypix, xpix = stat_gt[i]['ypix'], stat_gt[i]['xpix']\n",
    "    masks_gt[ypix, xpix] = i+1\n",
    "outlines_gt = masks_to_outlines(masks_gt)\n",
    "\n",
    "masks_all = []\n",
    "outlines_all = []\n",
    "for stat in stats:\n",
    "    masks = np.zeros((Ly, Lx), 'uint16')\n",
    "    for i in range(len(stat)):\n",
    "        ypix, xpix = stat[i]['ypix'], stat[i]['xpix']\n",
    "        masks[ypix, xpix] = i+1\n",
    "    outlines = outlines_list(masks)\n",
    "    masks_all.append(masks)\n",
    "    outlines_all.append(outlines)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ac9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/detect_metrics.npy', {\n",
    "    'max_proj': max_proj,  'cp_outlines': cp_outlines, 'dF_gt': dF_gt, \n",
    "    'neu_ex': neu_ex, 'ell_ex': ell_ex,\n",
    "'f_ex': f_ex, 'd_out': d_out, 'masks_gt': masks_gt,\n",
    "'outlines_gt': outlines_gt, 'outlines_all': outlines_all, 'tps': tps,\n",
    "'fps': fps, 'fns': fns, 'idef': idef\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import figures \n",
    "import importlib\n",
    "from fig_utils import *\n",
    "from cellpose import transforms\n",
    "importlib.reload(figures)\n",
    "\n",
    "fig = figures.detectmetrics_fig(max_proj, cp_outlines, dF_gt, neu_ex, \n",
    "                          ell_ex, f_ex, d_out,\n",
    "                          masks_gt, outlines_gt, outlines_all,\n",
    "                          tps, fps, fns, idef)\n",
    "fig.savefig('figures/fig5.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(figures)\n",
    "fig = figures.suppfig_detect(fns, fps)\n",
    "fig.savefig('figures/suppfig_detect.pdf', dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
